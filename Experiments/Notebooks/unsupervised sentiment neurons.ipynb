{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"unsupervised sentiment neurons.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"vCaHFSa3jCqW","colab_type":"code","outputId":"8ff7c777-6261-4836-ae54-2b28a541b30d","executionInfo":{"status":"ok","timestamp":1572065040381,"user_tz":-480,"elapsed":11191,"user":{"displayName":"Gan Jun Ying","photoUrl":"","userId":"05573927932906742162"}},"colab":{"base_uri":"https://localhost:8080/","height":131}},"source":["!git clone https://github.com/NVIDIA/sentiment-discovery.git\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cloning into 'sentiment-discovery'...\n","remote: Enumerating objects: 1133, done.\u001b[K\n","remote: Total 1133 (delta 0), reused 0 (delta 0), pack-reused 1133\u001b[K\n","Receiving objects: 100% (1133/1133), 55.98 MiB | 26.12 MiB/s, done.\n","Resolving deltas: 100% (698/698), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KMQjglrroIcb","colab_type":"code","outputId":"08f7952e-90b4-4dbf-fbed-0328df296b73","executionInfo":{"status":"ok","timestamp":1572065047462,"user_tz":-480,"elapsed":1000,"user":{"displayName":"Gan Jun Ying","photoUrl":"","userId":"05573927932906742162"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd/content/sentiment-discovery"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/sentiment-discovery\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2-RoBEpNmHXf","colab_type":"code","outputId":"ea497191-a82a-48ac-fcfc-d85e1a664c82","executionInfo":{"status":"ok","timestamp":1572065052402,"user_tz":-480,"elapsed":3929,"user":{"displayName":"Gan Jun Ying","photoUrl":"","userId":"05573927932906742162"}},"colab":{"base_uri":"https://localhost:8080/","height":148}},"source":["!rm -rf analysis,LICENSE, README.md, LICENSE\t\n","!ls #contents inside"],"execution_count":0,"outputs":[{"output_type":"stream","text":["analysis\t   figures\t\t   main.py\t    reparameterization\n","arguments.py\t   finetune_classifier.py  metric_utils.py  run_classifier.py\n","configure_data.py  fp16\t\t\t   model\t    script_docs\n","data\t\t   generate.py\t\t   multiproc.py     setup.py\n","data_utils\t   learning_rates.py\t   pretrain.py\t    threshold.py\n","experiments\t   logreg_utils.py\t   README.md\t    transfer.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CJtwZQxDoMp2","colab_type":"code","outputId":"6bb988bb-0fea-4bc3-c89e-8ab676a0f85f","executionInfo":{"status":"ok","timestamp":1572065074861,"user_tz":-480,"elapsed":20010,"user":{"displayName":"Gan Jun Ying","photoUrl":"","userId":"05573927932906742162"}},"colab":{"base_uri":"https://localhost:8080/","height":132}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"r0PKk29fM84r","colab_type":"text"},"source":["UPLOADING FILES FROM UR LOCAL FILE SYSTEM\n"]},{"cell_type":"code","metadata":{"id":"m0B903c4MqQ4","colab_type":"code","outputId":"1015b618-9268-4824-f0a0-5d5ad265ace2","executionInfo":{"status":"ok","timestamp":1572065098042,"user_tz":-480,"elapsed":14171,"user":{"displayName":"Gan Jun Ying","photoUrl":"","userId":"05573927932906742162"}},"colab":{"base_uri":"https://localhost:8080/","height":421}},"source":["#from google.colab import files\n","#uploaded=files.upload()\n","\n","\n","#from google.colab import drive\n","#drive.mount(‘/content/gdrive’)\n","#%cd gdrive/My Drive/project_folder/cloned_repo_folder\n","\n","! pip install unidecode\n","! pip install sentencepiece\n","! pip install emoji"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\r\u001b[K     |█▍                              | 10kB 16.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 3.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 2.7MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.1.1\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 2.8MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.83\n","Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n","\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42175 sha256=7630e38125309b5298d3e35765af6c05e68975dcd9bdf51805d7e4fcc42f5647\n","  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.5.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"26U4fyPmJaGK","colab_type":"code","colab":{}},"source":["! python setup.py\n","! python run_classifier.py --load_model ama_sst.pt  # what is ama_sst??  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OGVfRXogCoEa","colab_type":"text"},"source":["HOW TO USE THE FUNCTIONS AFTER CLONING???"]},{"cell_type":"code","metadata":{"id":"nz7Div-vtWkp","colab_type":"code","outputId":"16773580-a3ab-448e-96f7-7341b3bee4ce","executionInfo":{"status":"ok","timestamp":1572065267741,"user_tz":-480,"elapsed":8658,"user":{"displayName":"Gan Jun Ying","photoUrl":"","userId":"05573927932906742162"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["! python3 run_classifier.py --load_model ama_sst.pt                           # classify Binary SST\n","! python3 run_classifier.py --load_model ama_sst_16.pt --fp16                     # run classification in fp16\n","! python3 run_classifier.py --load_model ama_sst.pt --text-key withsentiment --data google_facebook_microsoft_news.csv    # classify your own dataset"],"execution_count":0,"outputs":[{"output_type":"stream","text":["usage: run_classifier.py [-h] [--model MODEL] [--lr LR]\n","                         [--constant-decay CONSTANT_DECAY] [--clip CLIP]\n","                         [--epochs EPOCHS] [--tied] [--seed SEED]\n","                         [--log-interval N] [--save SAVE] [--load LOAD]\n","                         [--load-optim] [--save-iters N] [--save-optim]\n","                         [--fp16] [--dynamic-loss-scale] [--no-weight-norm]\n","                         [--loss-scale LOSS_SCALE] [--world-size WORLD_SIZE]\n","                         [--distributed-backend DISTRIBUTED_BACKEND]\n","                         [--rank RANK] [--optim OPTIM] [--chkpt-grad]\n","                         [--multinode-init]\n","                         [--num-hidden-warmup NUM_HIDDEN_WARMUP]\n","                         [--emsize EMSIZE] [--nhid NHID] [--nlayers NLAYERS]\n","                         [--dropout DROPOUT] [--neural-alphabet]\n","                         [--alphabet-size ALPHABET_SIZE] [--ncontext NCONTEXT]\n","                         [--residuals] [--max-seq-len MAX_SEQ_LEN]\n","                         [--classifier-hidden-layers CLASSIFIER_HIDDEN_LAYERS [CLASSIFIER_HIDDEN_LAYERS ...]]\n","                         [--classifier-hidden-activation CLASSIFIER_HIDDEN_ACTIVATION]\n","                         [--classifier-dropout CLASSIFIER_DROPOUT]\n","                         [--all-layers] [--concat-max] [--concat-min]\n","                         [--concat-mean] [--get-hidden] [--neurons NEURONS]\n","                         [--heads-per-class HEADS_PER_CLASS] [--use-softmax]\n","                         [--double-thresh] [--dual-thresh]\n","                         [--joint-binary-train] [--data DATA [DATA ...]]\n","                         [--valid [VALID [VALID ...]]]\n","                         [--test [TEST [TEST ...]]]\n","                         [--process-fn {process_str,process_tweet}]\n","                         [--batch-size BATCH_SIZE]\n","                         [--eval-batch-size EVAL_BATCH_SIZE]\n","                         [--data-size DATA_SIZE] [--loose-json] [--preprocess]\n","                         [--delim DELIM]\n","                         [--non-binary-cols [NON_BINARY_COLS [NON_BINARY_COLS ...]]]\n","                         [--split SPLIT] [--text-key TEXT_KEY]\n","                         [--label-key LABEL_KEY]\n","                         [--eval-text-key EVAL_TEXT_KEY]\n","                         [--eval-label-key EVAL_LABEL_KEY]\n","                         [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer}]\n","                         [--tokenizer-model-type {bpe,char,unigram,word}]\n","                         [--vocab-size VOCAB_SIZE]\n","                         [--tokenizer-path TOKENIZER_PATH]\n","                         [--save_probs SAVE_PROBS]\n","                         [--write-results WRITE_RESULTS]\n","run_classifier.py: error: unrecognized arguments: --load_model ama_sst.pt\n","usage: run_classifier.py [-h] [--model MODEL] [--lr LR]\n","                         [--constant-decay CONSTANT_DECAY] [--clip CLIP]\n","                         [--epochs EPOCHS] [--tied] [--seed SEED]\n","                         [--log-interval N] [--save SAVE] [--load LOAD]\n","                         [--load-optim] [--save-iters N] [--save-optim]\n","                         [--fp16] [--dynamic-loss-scale] [--no-weight-norm]\n","                         [--loss-scale LOSS_SCALE] [--world-size WORLD_SIZE]\n","                         [--distributed-backend DISTRIBUTED_BACKEND]\n","                         [--rank RANK] [--optim OPTIM] [--chkpt-grad]\n","                         [--multinode-init]\n","                         [--num-hidden-warmup NUM_HIDDEN_WARMUP]\n","                         [--emsize EMSIZE] [--nhid NHID] [--nlayers NLAYERS]\n","                         [--dropout DROPOUT] [--neural-alphabet]\n","                         [--alphabet-size ALPHABET_SIZE] [--ncontext NCONTEXT]\n","                         [--residuals] [--max-seq-len MAX_SEQ_LEN]\n","                         [--classifier-hidden-layers CLASSIFIER_HIDDEN_LAYERS [CLASSIFIER_HIDDEN_LAYERS ...]]\n","                         [--classifier-hidden-activation CLASSIFIER_HIDDEN_ACTIVATION]\n","                         [--classifier-dropout CLASSIFIER_DROPOUT]\n","                         [--all-layers] [--concat-max] [--concat-min]\n","                         [--concat-mean] [--get-hidden] [--neurons NEURONS]\n","                         [--heads-per-class HEADS_PER_CLASS] [--use-softmax]\n","                         [--double-thresh] [--dual-thresh]\n","                         [--joint-binary-train] [--data DATA [DATA ...]]\n","                         [--valid [VALID [VALID ...]]]\n","                         [--test [TEST [TEST ...]]]\n","                         [--process-fn {process_str,process_tweet}]\n","                         [--batch-size BATCH_SIZE]\n","                         [--eval-batch-size EVAL_BATCH_SIZE]\n","                         [--data-size DATA_SIZE] [--loose-json] [--preprocess]\n","                         [--delim DELIM]\n","                         [--non-binary-cols [NON_BINARY_COLS [NON_BINARY_COLS ...]]]\n","                         [--split SPLIT] [--text-key TEXT_KEY]\n","                         [--label-key LABEL_KEY]\n","                         [--eval-text-key EVAL_TEXT_KEY]\n","                         [--eval-label-key EVAL_LABEL_KEY]\n","                         [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer}]\n","                         [--tokenizer-model-type {bpe,char,unigram,word}]\n","                         [--vocab-size VOCAB_SIZE]\n","                         [--tokenizer-path TOKENIZER_PATH]\n","                         [--save_probs SAVE_PROBS]\n","                         [--write-results WRITE_RESULTS]\n","run_classifier.py: error: unrecognized arguments: --load_model ama_sst_16.pt\n","usage: run_classifier.py [-h] [--model MODEL] [--lr LR]\n","                         [--constant-decay CONSTANT_DECAY] [--clip CLIP]\n","                         [--epochs EPOCHS] [--tied] [--seed SEED]\n","                         [--log-interval N] [--save SAVE] [--load LOAD]\n","                         [--load-optim] [--save-iters N] [--save-optim]\n","                         [--fp16] [--dynamic-loss-scale] [--no-weight-norm]\n","                         [--loss-scale LOSS_SCALE] [--world-size WORLD_SIZE]\n","                         [--distributed-backend DISTRIBUTED_BACKEND]\n","                         [--rank RANK] [--optim OPTIM] [--chkpt-grad]\n","                         [--multinode-init]\n","                         [--num-hidden-warmup NUM_HIDDEN_WARMUP]\n","                         [--emsize EMSIZE] [--nhid NHID] [--nlayers NLAYERS]\n","                         [--dropout DROPOUT] [--neural-alphabet]\n","                         [--alphabet-size ALPHABET_SIZE] [--ncontext NCONTEXT]\n","                         [--residuals] [--max-seq-len MAX_SEQ_LEN]\n","                         [--classifier-hidden-layers CLASSIFIER_HIDDEN_LAYERS [CLASSIFIER_HIDDEN_LAYERS ...]]\n","                         [--classifier-hidden-activation CLASSIFIER_HIDDEN_ACTIVATION]\n","                         [--classifier-dropout CLASSIFIER_DROPOUT]\n","                         [--all-layers] [--concat-max] [--concat-min]\n","                         [--concat-mean] [--get-hidden] [--neurons NEURONS]\n","                         [--heads-per-class HEADS_PER_CLASS] [--use-softmax]\n","                         [--double-thresh] [--dual-thresh]\n","                         [--joint-binary-train] [--data DATA [DATA ...]]\n","                         [--valid [VALID [VALID ...]]]\n","                         [--test [TEST [TEST ...]]]\n","                         [--process-fn {process_str,process_tweet}]\n","                         [--batch-size BATCH_SIZE]\n","                         [--eval-batch-size EVAL_BATCH_SIZE]\n","                         [--data-size DATA_SIZE] [--loose-json] [--preprocess]\n","                         [--delim DELIM]\n","                         [--non-binary-cols [NON_BINARY_COLS [NON_BINARY_COLS ...]]]\n","                         [--split SPLIT] [--text-key TEXT_KEY]\n","                         [--label-key LABEL_KEY]\n","                         [--eval-text-key EVAL_TEXT_KEY]\n","                         [--eval-label-key EVAL_LABEL_KEY]\n","                         [--tokenizer-type {CharacterLevelTokenizer,SentencePieceTokenizer}]\n","                         [--tokenizer-model-type {bpe,char,unigram,word}]\n","                         [--vocab-size VOCAB_SIZE]\n","                         [--tokenizer-path TOKENIZER_PATH]\n","                         [--save_probs SAVE_PROBS]\n","                         [--write-results WRITE_RESULTS]\n","run_classifier.py: error: unrecognized arguments: --load_model ama_sst.pt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Syq7eTP8p1zO","colab_type":"code","outputId":"f35814a0-1591-4989-b430-ef092c7cebd8","executionInfo":{"status":"error","timestamp":1571997193017,"user_tz":-480,"elapsed":886,"user":{"displayName":"Gan Jun Ying","photoUrl":"","userId":"05573927932906742162"}},"colab":{"base_uri":"https://localhost:8080/","height":457}},"source":["! python3 pretrain.py                                                               #train a large model on imdb\n","! python3 pretrain.py --model LSTM --nhid 512                                       #train a small LSTM instead\n","! python3 pretrain.py --fp16 --dynamic-loss-scale                                   #train a model with fp16\n","! python3 -m multiproc pretrain.py                                                  #distributed model training\n","! python3 pretrain.py --data ./data/amazon/reviews.json --lazy --loose-json --text-key reviewText --label-key overall --optim Adam --split 1000,1,1 \n","! python3 pretrain.py --tokenizer-type SentencePieceTokenizer --vocab-size 32000 --tokenizer-type bpe --tokenizer-path ama_32k_tokenizer.model \n","! python3 pretrain.py --tokenizer-type SentencePieceTokenizer --vocab-size 32000  --decoder-layers 12 --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-learned-pos --decoder-attention-heads 8\n","! bash ./experiments/train_mlstm_singlenode.sh                                      #run our mLSTM training script on 1 DGX-1V\n","! bash ./experiments/train_transformer_singlenode.sh                                #run our transformer training script on 1 DGX-1V "],"execution_count":0,"outputs":[{"output_type":"stream","text":["configuring data\n","Creating mlstm\n","* number of parameters: 86298945\n","decaying None\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n","THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=50 error=38 : no CUDA-capable device is detected\n","Exception in thread Thread-1:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n","    self.run()\n","  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/content/sentiment-discovery/data_utils/loaders.py\", line 318, in _shardloader_pin_memory_loop\n","    batch = pin_memory_batch(batch)\n","  File \"/content/sentiment-discovery/data_utils/loaders.py\", line 87, in pin_memory_batch\n","    return [pin_memory_batch(sample) for sample in batch]\n","  File \"/content/sentiment-discovery/data_utils/loaders.py\", line 87, in <listcomp>\n","    return [pin_memory_batch(sample) for sample in batch]\n","  File \"/content/sentiment-discovery/data_utils/loaders.py\", line 81, in pin_memory_batch\n","    return batch.pin_memory()\n","RuntimeError: cuda runtime error (38) : no CUDA-capable device is detected at /pytorch/aten/src/THC/THCGeneral.cpp:50\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"lCjNSRPmCe3D","colab_type":"code","colab":{}},"source":["!python3 transfer.py --load mlstm.pt                                 #performs transfer to SST, saves results to `<model>_transfer/` directory\n","!python3 transfer.py --load mlstm.pt --neurons 5                     #use 5 neurons for the second regression\n","!python3 transfer.py --load mlstm.pt --fp16                          #run model in fp16 for featurization step\n","!bash ./experiments/run_sk_sst.sh                                    #run transfer learning with mlstm on imdb dataset\n","!bash ./experiments/run_sk_imdb.sh                                   #run transfer learning with mlstm on sst dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oHCm_hCvCkXL","colab_type":"code","colab":{}},"source":["!python3 finetune_classifier.py --load mlstm.pt --lr 2e-5 --aux-lm-loss --aux-lm-loss-weight .02   #finetune mLSTM model on sst (default dataset) with auxiliary loss\n","!python3 finetune_classifier.py --load mlstm.pt --automatic-thresholding --threshold-metric f1     #finetune mLSTM model on sst and automatically select classification thresholds based on the validation f1 score\n","!python3 finetune_classifier.py --tokenizer-type SentencePieceTokenizer --vocab-size 32000 --tokenizer-type bpe tokenizer-path ama_32k_tokenizer.model --model transformer --lr 2e-5 --decoder-layers 12 --decoder-embed-dim 768 --decoder-ffn-embed-dim 3072 --decoder-learned-pos --decoder-attention-heads 8 --load transformer.pt --use-final-embed\n","! python3 finetune_classifier.py --automatic-thresholding --non-binary-cols l1 l2 l3 --lr 2e-5--classifier-hidden-layers 2048 1024 3 --heads-per-class 4 --aux-head-variance-loss-weight 1. --data <custom_train>.csv --val <custom_val>.csv --test <custom_test>.csv --load mlstm.pt\n","!bash ./experiments/se_transformer_multihead.sh                                                    #finetune a multihead transformer on 8 semeval categories"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5WE0-UO7AF4C","colab_type":"text"},"source":["---------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"NegUP7Q7GLJW","colab_type":"text"},"source":["----------------------------------------------------------------"]},{"cell_type":"code","metadata":{"id":"3rjLYpvHA-TB","colab_type":"code","outputId":"21d32b84-6a9d-4f2c-e75a-aee0ca1c8600","executionInfo":{"status":"ok","timestamp":1572421611734,"user_tz":-480,"elapsed":23731,"user":{"displayName":"Gan Jun Ying","photoUrl":"","userId":"05573927932906742162"}},"colab":{"base_uri":"https://localhost:8080/","height":134}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TlhWtfxGGqX1","colab_type":"code","outputId":"144fbd8c-5146-4e63-b496-abb30900c94b","executionInfo":{"status":"ok","timestamp":1572421758868,"user_tz":-480,"elapsed":4423,"user":{"displayName":"Gan Jun Ying","photoUrl":"","userId":"05573927932906742162"}},"colab":{"base_uri":"https://localhost:8080/","height":114}},"source":["! git clone https://github.com/jonnykira/Tensorflow_mLSTM.git\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Cloning into 'Tensorflow_mLSTM'...\n","remote: Enumerating objects: 71, done.\u001b[K\n","remote: Total 71 (delta 0), reused 0 (delta 0), pack-reused 71\u001b[K\n","Unpacking objects: 100% (71/71), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M5NykUpjKYV2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6af4fd4d-b4ea-415a-9805-025e62161983","executionInfo":{"status":"ok","timestamp":1572421834941,"user_tz":-480,"elapsed":1045,"user":{"displayName":"Gan Jun Ying","photoUrl":"","userId":"05573927932906742162"}}},"source":["cd/content/Tensorflow_mLSTM"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/content/Tensorflow_mLSTM\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DuNN6L2PBAQe","colab_type":"code","outputId":"a1799512-154c-480d-be08-67e6957e9734","executionInfo":{"status":"ok","timestamp":1572396810620,"user_tz":-480,"elapsed":54276,"user":{"displayName":"Gan Jun Ying","photoUrl":"","userId":"05573927932906742162"}},"colab":{"base_uri":"https://localhost:8080/","height":484}},"source":["! pip install caffeine # to make the computer not sleep \n","! python train_mLSTM.py --data_dir=\"/content/drive/My Drive/Colab/datasets/google_facebook_microsoft_news.txt\"\n","! tensorboard --logdir=path/to/--data_dir=\"/content/drive/My Drive/Colab/datasets/results\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting caffeine\n","  Downloading https://files.pythonhosted.org/packages/66/ba/acd2d55744d86142c533c6c73412c5aa0387467f8f75aa3d62a86c866fe8/caffeine-0.5.tar.gz\n","Building wheels for collected packages: caffeine\n","  Building wheel for caffeine (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for caffeine: filename=caffeine-0.5-cp36-none-any.whl size=2315 sha256=d21038c51a1f8e1b0200b331a1146e5c9bb2262a46cc1fabb048ef90a0384467\n","  Stored in directory: /root/.cache/pip/wheels/3b/f3/1b/4d6dfaf0185f1370a37c418c62e4fd071c5335ad295386970c\n","Successfully built caffeine\n","Installing collected packages: caffeine\n","Successfully installed caffeine-0.5\n","/usr/local/lib/python3.6/dist-packages/caffeine.py:77: UserWarning: This package is designed for use on Mac OS X; use on other systems may cause errors and will probably fail.\n","  warnings.warn('This package is designed for use on Mac OS X; use on other systems may cause errors and will probably fail.')\n","Traceback (most recent call last):\n","  File \"train_mLSTM.py\", line 12, in <module>\n","    import caffeine\n","  File \"/usr/local/lib/python3.6/dist-packages/caffeine.py\", line 99, in <module>\n","    on()\n","  File \"/usr/local/lib/python3.6/dist-packages/caffeine.py\", line 88, in on\n","    _caf = subprocess.Popen(['caffeinate', '-is', '-w', str(_pid)])\n","  File \"/usr/lib/python3.6/subprocess.py\", line 729, in __init__\n","    restore_signals, start_new_session)\n","  File \"/usr/lib/python3.6/subprocess.py\", line 1364, in _execute_child\n","    raise child_exception_type(errno_num, err_msg, err_filename)\n","FileNotFoundError: [Errno 2] No such file or directory: 'caffeinate': 'caffeinate'\n","TensorBoard 1.15.0 at http://9d113d18a121:6006/ (Press CTRL+C to quit)\n"],"name":"stdout"}]}]}